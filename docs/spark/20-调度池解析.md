[TOC]

# spark 任务调度池的解析

本篇看一下spark调度池的实现原理。

## 调度池的初始化

初始化：

> org.apache.spark.scheduler.TaskSchedulerImpl#initialize

```java
// taskScheduler 先进行 initialize 在进行 start
def initialize(backend: SchedulerBackend) {
    // 记录传递进来的 backend
    this.backend = backend
        // 根据不同的模式,来创建调度器
        schedulableBuilder = {
        schedulingMode match {
            case SchedulingMode.FIFO =>
                new FIFOSchedulableBuilder(rootPool)
                case SchedulingMode.FAIR =>
                new FairSchedulableBuilder(rootPool, conf)
                case _ =>
                throw new IllegalArgumentException(s"Unsupported $SCHEDULER_MODE_PROPERTY: " +
                                                   s"$schedulingMode")
        }
    }
    // 对调度器的一些工作
    schedulableBuilder.buildPools()
}
```

在这里呢，根据设置的不同的调度模式，来创建不同的builder，针对两个builder的buildPools都看一下：

```scala
private[spark] class FIFOSchedulableBuilder(val rootPool: Pool)
extends SchedulableBuilder with Logging {

    override def buildPools() {
        // nothing
    }

    override def addTaskSetManager(manager: Schedulable, properties: Properties) {
        rootPool.addSchedulable(manager)
    }
}
```

```scala
private[spark] class FairSchedulableBuilder(val rootPool: Pool, conf: SparkConf)
  extends SchedulableBuilder with Logging {
  // 设置调度器的配置文件
  val SCHEDULER_ALLOCATION_FILE_PROPERTY = "spark.scheduler.allocation.file"
  val schedulerAllocFile = conf.getOption(SCHEDULER_ALLOCATION_FILE_PROPERTY)
  // 默认的 公平调度的 配置文件名
  val DEFAULT_SCHEDULER_FILE = "fairscheduler.xml"
  // 调度池的属性name
  val FAIR_SCHEDULER_PROPERTIES = "spark.scheduler.pool"
  // 默认的 pool 的名字
  val DEFAULT_POOL_NAME = "default"
  // 最小的 cpu数
  val MINIMUM_SHARES_PROPERTY = "minShare"
  // 调度模式
  val SCHEDULING_MODE_PROPERTY = "schedulingMode"
  // 权重
  val WEIGHT_PROPERTY = "weight"
  val POOL_NAME_PROPERTY = "@name"
  val POOLS_PROPERTY = "pool"
  val DEFAULT_SCHEDULING_MODE = SchedulingMode.FIFO
  val DEFAULT_MINIMUM_SHARE = 0
  val DEFAULT_WEIGHT = 1

  override def buildPools() {
    var fileData: Option[(InputStream, String)] = None
    try {
      // 获取 调用配置文件中的配置
      fileData = schedulerAllocFile.map { f =>
        val fis = new FileInputStream(f)
        logInfo(s"Creating Fair Scheduler pools from $f")
        Some((fis, f))
      }.getOrElse {
        // 获取默认的 配置文件 fairscheduler.xml
        val is = Utils.getSparkClassLoader.getResourceAsStream(DEFAULT_SCHEDULER_FILE)
        if (is != null) {
          logInfo(s"Creating Fair Scheduler pools from default file: $DEFAULT_SCHEDULER_FILE")
          Some((is, DEFAULT_SCHEDULER_FILE))
        } else {
          logWarning("Fair Scheduler configuration file not found so jobs will be scheduled in " +
            s"FIFO order. To use fair scheduling, configure pools in $DEFAULT_SCHEDULER_FILE or " +
            s"set $SCHEDULER_ALLOCATION_FILE_PROPERTY to a file that contains the configuration.")
          None
        }
      }
    //  创建公平调度的 pool
      fileData.foreach { case (is, fileName) => buildFairSchedulerPool(is, fileName) }
    } catch {
      case NonFatal(t) =>
        val defaultMessage = "Error while building the fair scheduler pools"
        val message = fileData.map { case (is, fileName) => s"$defaultMessage from $fileName" }
          .getOrElse(defaultMessage)
        logError(message, t)
        throw t
    } finally {
      fileData.foreach { case (is, fileName) => is.close() }
    }
    // finally create "default" pool
    buildDefaultPool()
  }
```

这里看一看到读取fairscheduler.xml文件中的内容来创建pool，最后也创建了一个defaultpool。

配置文件：

```xml
<allocations>
   <pool name="production">
   <schedulingMode>FAIR</schedulingMode>
   <weight>1</weight>
   <minShare>2</minShare>
   </pool>
   <pool name="test">
   <schedulingMode>FIFO</schedulingMode>
   <weight>2</weight>
   <minShare>3</minShare>
   </pool>
</allocations>
```

> org.apache.spark.scheduler.FairSchedulableBuilder#buildFairSchedulerPool

```scala
private def buildFairSchedulerPool(is: InputStream, fileName: String) {
    val xml = XML.load(is)
    for (poolNode <- (xml \\ POOLS_PROPERTY)) {
        // 获取pool的名字
        val poolName = (poolNode \ POOL_NAME_PROPERTY).text
        //  获取调度的模式
        val schedulingMode = getSchedulingModeValue(poolNode, poolName,DEFAULT_SCHEDULING_MODE, fileName)
        // 获取最小的 资源数 cpu 核数
        val minShare = getIntValue(poolNode, poolName, MINIMUM_SHARES_PROPERTY,DEFAULT_MINIMUM_SHARE, fileName)
        // 权重
        val weight = getIntValue(poolNode, poolName, WEIGHT_PROPERTY,DEFAULT_WEIGHT, fileName)
        // 添加一个 子pool到 rootPool中
        rootPool.addSchedulable(new Pool(poolName, schedulingMode, minShare, weight))
        logInfo("Created pool: %s, schedulingMode: %s, minShare: %d, weight: %d".format(
            poolName, schedulingMode, minShare, weight))
    }
}
```

默认pool的创建

> org.apache.spark.scheduler.FairSchedulableBuilder#buildDefaultPool

```scala
  private def buildDefaultPool() {
    if (rootPool.getSchedulableByName(DEFAULT_POOL_NAME) == null) {
      val pool = new Pool(DEFAULT_POOL_NAME, DEFAULT_SCHEDULING_MODE,
        DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)
      rootPool.addSchedulable(pool)
      logInfo("Created default pool: %s, schedulingMode: %s, minShare: %d, weight: %d".format(
        DEFAULT_POOL_NAME, DEFAULT_SCHEDULING_MODE, DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT))
    }
  }
```

这里就看到fair在初始化时，其队列中存储的是pool，而且其会创建一个名字为default的pool，还有解析fairscheduler.xml中的内容来创建pool。

由此可以看出一个FIFO和Fair pool的区别：

1. FIFO pool中只有一个rootPool，其任务队列中直接存储的就是TaskSetManager
2. Fair pool中rootPool中的任务队列中存储的是其他名字的other pool，而other pool中存储的是具体的TaskSetManager；且other pool使用FIFO算法

## 提交任务到调度池



> org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks

```scala
  // 提交任务
  override def submitTasks(taskSet: TaskSet) {
    val tasks = taskSet.tasks
    logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
    this.synchronized {
      // 针对 taskSet 创建 taskSetManager
      val manager = createTaskSetManager(taskSet, maxTaskFailures)
      val stage = taskSet.stageId
      val stageTaskSets =
        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])

      // Mark all the existing TaskSetManagers of this stage as zombie, as we are adding a new one.
      // This is necessary to handle a corner case. Let's say a stage has 10 partitions and has 2
      // TaskSetManagers: TSM1(zombie) and TSM2(active). TSM1 has a running task for partition 10
      // and it completes. TSM2 finishes tasks for partition 1-9, and thinks he is still active
      // because partition 10 is not completed yet. However, DAGScheduler gets task completion
      // events for all the 10 partitions and thinks the stage is finished. If it's a shuffle stage
      // and somehow it has missing map outputs, then DAGScheduler will resubmit it and create a
      // TSM3 for it. As a stage can't have more than one active task set managers, we must mark
      // TSM2 as zombie (it actually is).
      stageTaskSets.foreach { case (_, ts) =>
        ts.isZombie = true
      }
      stageTaskSets(taskSet.stageAttemptId) = manager
      // 添加任务到队列中,来等待task调度执行
      // 添加任务到 pool中
      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
      if (!isLocal && !hasReceivedTask) {
        starvationTimer.scheduleAtFixedRate(new TimerTask() {
          override def run() {
            if (!hasLaunchedTask) {
              logWarning("Initial job has not accepted any resources; " +
                "check your cluster UI to ensure that workers are registered " +
                "and have sufficient resources")
            } else {
              this.cancel()
            }
          }
        }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)
      }
      hasReceivedTask = true
    }
    // 重点 ...
    // 分发task 到各个 executor
    backend.reviveOffers()
  }
```

这里具体的提交任务，步骤如下：

1. 根据要提交的taskSet创建 taskSetManager
2. 把此taskSetManager提交到 pool中
3. 开始一次任务调度

这里看一下提交manager到pool中：

> org.apache.spark.scheduler.FIFOSchedulableBuilder#addTaskSetManager

```scala
  override def addTaskSetManager(manager: Schedulable, properties: Properties) {
    rootPool.addSchedulable(manager)
  }
```

```scala
  override def addSchedulable(schedulable: Schedulable) {
    require(schedulable != null)
    schedulableQueue.add(schedulable)
    schedulableNameToSchedulable.put(schedulable.name, schedulable)
    schedulable.parent = this
  }
```

这里可以看到，在FIFO算法中，就是直接把taskSetManager放入到 schedulableQueue中。

> org.apache.spark.scheduler.FairSchedulableBuilder#addTaskSetManager

```scala
  val FAIR_SCHEDULER_PROPERTIES = "spark.scheduler.pool"
  // 默认的 pool 的名字
  val DEFAULT_POOL_NAME = "default"  
override def addTaskSetManager(manager: Schedulable, properties: Properties) {
    val poolName = if (properties != null) {
        properties.getProperty(FAIR_SCHEDULER_PROPERTIES, DEFAULT_POOL_NAME)
      } else {
        DEFAULT_POOL_NAME
      }
    // 获取此 poolName对应的pool中
    // 没有获取到,则创建一个新的pool 并 添加到 rootpool中
    // 获取到了,则直接把 taskSetManager 添加到此 pool中
    var parentPool = rootPool.getSchedulableByName(poolName)
    if (parentPool == null) {
      // we will create a new pool that user has configured in app
      // instead of being defined in xml file
      parentPool = new Pool(poolName, DEFAULT_SCHEDULING_MODE,
        DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)
      rootPool.addSchedulable(parentPool)
      logWarning(s"A job was submitted with scheduler pool $poolName, which has not been " +
        "configured. This can happen when the file that pools are read from isn't set, or " +
        s"when that file doesn't contain $poolName. Created $poolName with default " +
        s"configuration (schedulingMode: $DEFAULT_SCHEDULING_MODE, " +
        s"minShare: $DEFAULT_MINIMUM_SHARE, weight: $DEFAULT_WEIGHT)")
    }
    // 添加到 poolName对应的pool中
    parentPool.addSchedulable(manager)
    logInfo("Added task set " + manager.name + " tasks to pool " + poolName)
  }
```

可以看到这里的添加操作，先获取FAIR_SCHEDULER_PROPERTIES的名字，没有则使用默认的poolName：default；之后获取此名字对应的pool，然后把此taskSetManager添加到对应的此name对应的pool中，否则添加到名字为default的pool中。

## 从调度池中获取任务

提交完任务后，会进行一次任务的调度：

> org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#receive

```scala
   // driver endpoint的消息接收
    override def receive: PartialFunction[Any, Unit] = {
      // 发送task到worker执行
        // 当提交任务时, driver也会向自己发送 ReviveOffers消息
      case ReviveOffers =>
        makeOffers()
        ....
        
    }
```

> org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#makeOffers

```java
    // Make fake resource offers on all executors
    // 尝试把 待处理的任务 发送到 executor
    private def makeOffers() {
      // Make sure no executor is killed while some task is launching on it
      val taskDescs = withLock {
        // Filter out executors under killing
        // 过滤出 active executor
        val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
        // 此把 所有的 alive的 executor 包装为 WorkerOffer
        val workOffers = activeExecutors.map {
          case (id, executorData) =>
            new WorkerOffer(id, executorData.executorHost, executorData.freeCores,
              Some(executorData.executorAddress.hostPort))
        }.toIndexedSeq
        // 重点 --
        // 这里 遍历所有的workOffers,即所有的 executor来获取 具体到把任务发送到那个 executor上
        scheduler.resourceOffers(workOffers)
      }
      // 如果存在任务,则启动 task执行任务
      // 让 executor 启动task
      if (!taskDescs.isEmpty) {
        // 向executor 发送 启动task的消息
        launchTasks(taskDescs)
      }
    }
```

> org.apache.spark.scheduler.TaskSchedulerImpl#resourceOffers

```java
// 总体来说此函数还是比较长的
// 1. 首先获取任务
// 2. 在根据 任务的 local level来获取任务
def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
	......
    // 黑名单 更新
    blacklistTrackerOpt.foreach(_.applyBlacklistTimeout())
  // 使用 黑名单 对 alive的 executor 和host 进行过滤
    val filteredOffers = blacklistTrackerOpt.map { blacklistTracker =>
      offers.filter { offer =>
        !blacklistTracker.isNodeBlacklisted(offer.host) &&
          !blacklistTracker.isExecutorBlacklisted(offer.executorId)
      }
    }.getOrElse(offers)
    // shuffleOffers 把 filteredOffers 打乱，避免任务 集中在某台机器
    val shuffledOffers = shuffleOffers(filteredOffers)
    // Build a list of tasks to assign to each worker.
    val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores / CPUS_PER_TASK))
    val availableCpus = shuffledOffers.map(o => o.cores).toArray
    // 获取要执行的任务; 根据不同的调度算法，最终的执行也 不相同
    val sortedTaskSets = rootPool.getSortedTaskSetQueue
    for (taskSet <- sortedTaskSets) {
      logDebug("parentName: %s, name: %s, runningTasks: %s".format(
        taskSet.parent.name, taskSet.name, taskSet.runningTasks))
      if (newExecAvail) {
        // 计算 taskSet的本地性 ??
        taskSet.executorAdded()
      }
    }
    for (taskSet <- sortedTaskSets) {
      val availableSlots = availableCpus.map(c => c / CPUS_PER_TASK).sum
      // Skip the barrier taskSet if the available slots are less than the number of pending tasks.
      if (taskSet.isBarrier && availableSlots < taskSet.numTasks) {
        logInfo(s"Skip current round of resource offers for barrier stage ${taskSet.stageId} " +
          s"because the barrier taskSet requires ${taskSet.numTasks} slots, while the total " +
          s"number of available slots is $availableSlots.")
      } else {
        var launchedAnyTask = false
        // Record all the executor IDs assigned barrier tasks on.
        val addressesWithDescs = ArrayBuffer[(String, TaskDescription)]()
        // 计算 task的 本地性, 并把 taskSet中的任务 放入到 tasks中
        for (currentMaxLocality <- taskSet.myLocalityLevels) {
          var launchedTaskAtCurrentMaxLocality = false
          do {
            // resourceOfferSingleTaskSet 需要要运行的任务
            launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet(taskSet,
              currentMaxLocality, shuffledOffers, availableCpus, tasks, addressesWithDescs)
            launchedAnyTask |= launchedTaskAtCurrentMaxLocality
          } while (launchedTaskAtCurrentMaxLocality)
        }
		......
    // TODO SPARK-24823 Cancel a job that contains barrier stage(s) if the barrier tasks don't get
    // launched within a configured time.
    if (tasks.size > 0) {
      hasLaunchedTask = true
    }
    return tasks
  }
```

此函数比较长，总体来说做了两件事：

1. 从pool中去获取任务
2. 根据任务的 local level(本地性)，按照 preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY 来获取任务。

这里主要看一下任务的获取：

> org.apache.spark.scheduler.Pool#getSortedTaskSetQueue

```java
 // 获取排序后的任务的队列
  override def getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] = {
    val sortedTaskSetQueue = new ArrayBuffer[TaskSetManager]
    // 此处对 task 进行排序操作
    // 如果是FIFO, 则优先级小的先运行,优先级相等 stageId 小的先运行
    // 如果是 fair
    // 1. 谁需要的资源少,谁先运行
    // 2. 谁要运行的任务少,则先运行
    // 3. 谁的 shareRatio 小,则先运行
    // 4. 谁的 tashWeight 小,则先运行
    // 5. 谁的名字小,则先运行

    // 如果是 fifo 来获取任务,那么这里排序完成后,就直接返回的是 taskManager
    // 如果是 fair 来获取任务,那么这里拍完序后, 返回的是 pool
    val sortedSchedulableQueue =
      schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)
    for (schedulable <- sortedSchedulableQueue) {
      // 如果是fifo, 那么这里就直接返回了 taskSetManager了
      // 如果是fair,那么这里是再次调用  pool的getSortedTaskSetQueue方法, 并在pool的内部再次进行排序
      // 也就是说fair内部是pool, pool内部再次使用 fifo算法来 获取要执行的任务
      sortedTaskSetQueue ++= schedulable.getSortedTaskSetQueue
    }
    sortedTaskSetQueue
  }
```

这里的注释写的还是比较清楚了，针对不同的mode下的pool，获取资源时是不一样的：

1. Fifo中，schedulable.getSortedTaskSetQueue直接获取按照fifo算法排序后的 taskSetManager
2. Fair中，schedulable.getSortedTaskSetQueue获取的是按照fair算法排序后的pool，获取到pool后，再次调用此pool的getSortedTaskSetQueue方法，此时才获取到使用FIFO算法排序后的taskSetManager。

看一下这两个mode下的排序方式：

> FIFO

```scala
private[spark] class FIFOSchedulingAlgorithm extends SchedulingAlgorithm {
  override def comparator(s1: Schedulable, s2: Schedulable): Boolean = {
    // 获取各自的 priority 其就是 jobId
    val priority1 = s1.priority
    val priority2 = s2.priority
    // 比较这两个优先级的大小
    // res = 1   pri1  大于 pri2
    // res = -1  pri1 小于 pri2
    // res = 0  相等
    var res = math.signum(priority1 - priority2)
    if (res == 0) {
      // 如果两个任务的优先级相等, 则比较其 stageId的大小
      val stageId1 = s1.stageId
      val stageId2 = s2.stageId
      res = math.signum(stageId1 - stageId2)
    }
    res < 0
  }
}
```

由此可见是先按照jobId，之后按照stageId，也就是先提交的任务先运行。

> fair

```scala
private[spark] class FairSchedulingAlgorithm extends SchedulingAlgorithm {
  override def comparator(s1: Schedulable, s2: Schedulable): Boolean = {
    // 最小数 资源数 -- cpu 核数
    val minShare1 = s1.minShare
    val minShare2 = s2.minShare
    // 运行的task 数
    val runningTasks1 = s1.runningTasks
    val runningTasks2 = s2.runningTasks
    // 任务数 小于 最小资源数,则需要调度
    val s1Needy = runningTasks1 < minShare1
    val s2Needy = runningTasks2 < minShare2
    // 最小分配的 比率
    val minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, 1.0)
    val minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, 1.0)
    val taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble
    val taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble

    var compare = 0
    // 如果 s1 需要调度,s2不需要,则返回true
    if (s1Needy && !s2Needy) {
      return true
      // s2需要,返回false
    } else if (!s1Needy && s2Needy) {
      return false
    } else if (s1Needy && s2Needy) {
      // s1和s2都需要,则比较 shareRatio
      compare = minShareRatio1.compareTo(minShareRatio2)
    } else {
      // 否则比较 taskWeightRatio
      compare = taskToWeightRatio1.compareTo(taskToWeightRatio2)
    }
    // shareRatio1 小 或者  taskWeight1 小,则返回true
    if (compare < 0) {
      true
    } else if (compare > 0) {
      // ratio2 大,则返回false
      false
    } else {
      // 都相等,则比较名字
      s1.name < s2.name
    }
  }
```

此处的就考虑的因素比较多，总体来说占用资源少的先运行。

spark的资源pool，就分析到这里了。







































































