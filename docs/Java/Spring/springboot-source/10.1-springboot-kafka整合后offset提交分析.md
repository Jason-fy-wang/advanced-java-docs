[TOC]

# springboot-kafka整合后offset提交分析

首先看一下手动提交offset的配置，以及使用:

```java
    @Bean
    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String,String>>
    kafkaListenerContainerFactory(){
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConcurrency(10);
        factory.setConsumerFactory(consumerFactory());
        factory.getContainerProperties().setPollTimeout(1500);
        factory.setBatchListener(true);
        // 设置手动提交 
        factory.getContainerProperties().setAckMode(AbstractMessageListenerContainer.AckMode.MANUAL);
        return factory;
    }
```

```java
  @KafkaListener(topics = {"test"},containerFactory = "containerFactory")
    public void kafkaConsumer(List<ConsumerRecord<String,String>> records, Acknowledgment ack){
     	// 业务处理
        process(records);
        ack.acknowledge();
    }
```

看一下这个配置，以及ack.acknowledge动作到底做了什么工作吧。

提交offset的模式：

```java
public enum AckMode {
	// 每处理一条就提交一条
    RECORD,
   // 批量提交
    BATCH,
    // 按照时间提交
    TIME,
    // 按照数量提交
    COUNT,
    // 按照时间和数量提交,两个条件达到其一 就提交
    COUNT_TIME,
	// 手动提交
    MANUAL,
    // 手动立即提交
    MANUAL_IMMEDIATE,
}
```

先看一下ack.acknowledge动作:

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer.ConsumerBatchAcknowledgment#acknowledge

```java
// 手动提交  offset 操作
@Override
public void acknowledge() {
    Assert.state(ListenerConsumer.this.isAnyManualAck,
                 "A manual ackmode is required for an acknowledging listener");
    for (ConsumerRecord<K, V> record : getHighestOffsetRecords(this.records)) {
        processAck(record);
    }
}
```

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#getHighestOffsetRecords

```java
private Collection<ConsumerRecord<K, V>> getHighestOffsetRecords(List<ConsumerRecord<K, V>> records) {
    final Map<TopicPartition, ConsumerRecord<K, V>> highestOffsetMap = new HashMap<>();
    records.forEach(r -> {
        highestOffsetMap.compute(new TopicPartition(r.topic(), r.partition()),
                                 (k, v) -> v == null ? r : r.offset() > v.offset() ? r : v);
    });
    return highestOffsetMap.values();
}
```

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#processAck

```java
// 保存ack的信息
private final BlockingQueue<ConsumerRecord<K, V>> acks = new LinkedBlockingQueue<>();
private void processAck(ConsumerRecord<K, V> record) {
    // 如果当前线程不是消费线程,则仍然保存起来
    if (!Thread.currentThread().equals(this.consumerThread)) {
        try {
            this.acks.put(record);
        }
        catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new KafkaException("Interrupted while storing ack", e);
        }
    }
    else { // 如果是立即提交offet
        if (this.isManualImmediateAck) {
            try {
                // 则立即进行提交
                ackImmediate(record);
            }
            catch (WakeupException e) {
                // ignore - not polling
            }
        }
        else {
            // 如果不是立即ack,则把此record对应的topic的分区的offset信息保存起来
            addOffset(record);
        }
    }
}
```

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#addOffset

```java
// 保存 topic对应的分区offset信息
private final Map<String, Map<Integer, Long>> offsets = new HashMap<>();
private void addOffset(ConsumerRecord<K, V> record) {
    // 添加offset
    this.offsets.computeIfAbsent(record.topic(), v -> new ConcurrentHashMap<>())
        .compute(record.partition(), (k, v) -> v == null ? record.offset() : Math.max(v, record.offset()));
}
```

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#ackImmediate

```java
private void ackImmediate(ConsumerRecord<K, V> record) {
    // 创建  topic分区 及其 对应的offset
    Map<TopicPartition, OffsetAndMetadata> commits = Collections.singletonMap(
        new TopicPartition(record.topic(), record.partition()),
        new OffsetAndMetadata(record.offset() + 1));
    // 打印
    this.commitLogger.log(() -> "Committing: " + commits);
    // 如果是同步提交
    if (this.containerProperties.isSyncCommits()) {
        // 则进行同步提交
        this.consumer.commitSync(commits);
    }
    else {
        // 异步提交
        // 并设置了提交完成的回调方法
        this.consumer.commitAsync(commits, this.commitCallback);
    }
}
```

由此可见，除非是立即提交，不然仍然是吧offset 保存起来，并没有进行提交。



看一下listenerConsumer中的处理:

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#run

```java
@Override
public void run() {
    ....// 省略
    while (isRunning()) {
        try {
            // 对提交的操作
            if (!this.autoCommit && !this.isRecordAck) {
                // 对offset的提交
                // -- 重点---
                // 可见手动提交 其实也并不是立即进行了提交操作, 同样也是进行了保存
                processCommits();
            }
            // offset的修改
            processSeeks();
            if (!this.consumerPaused && isPaused()) {
                // 暂停消费
                this.consumer.pause(this.consumer.assignment());
                this.consumerPaused = true;
                this.logger.debug("Paused consumption from: " + this.consumer.paused());
            }
            //  发布 ConsumerPausedEvent 消息
            publishConsumerPausedEvent(this.consumer.assignment());
        }
        this.polling.set(true);
        // 消费消息
        ConsumerRecords<K, V> records = this.consumer.poll(this.containerProperties.getPollTimeout());
        if (!this.polling.compareAndSet(true, false)) {
            if (records.count() > 0 && this.logger.isDebugEnabled()) {
                this.logger.debug("Discarding polled records, container stopped: " + records.count());
            }
            break;
        }
        // 更新时间
        this.lastPoll = System.currentTimeMillis();
        // 恢复消费
        if (this.consumerPaused && !isPaused()) {
            if (this.logger.isDebugEnabled()) {
                this.logger.debug("Resuming consumption from: " + this.consumer.paused());
            }
            Set<TopicPartition> paused = this.consumer.paused();
            this.consumer.resume(paused);
            this.consumerPaused = false;
            publishConsumerResumedEvent(paused);
        }
        // 消息打印
        if (records != null && this.logger.isDebugEnabled()) {
            this.logger.debug("Received: " + records.count() + " records");
            if (records.count() > 0 && this.logger.isTraceEnabled()) {
                this.logger.trace(records.partitions().stream()
                                  .flatMap(p -> records.records(p).stream())
                                  // map to same format as send metadata toString()
                                  .map(r -> r.topic() + "-" + r.partition() + "@" + r.offset())
                                  .collect(Collectors.toList()));
            }
        }
        if (records != null && records.count() > 0) {
            if (this.containerProperties.getIdleEventInterval() != null) {
                lastReceive = System.currentTimeMillis();
            }
            // todo  调用用户的方法  具体进行处理
            invokeListener(records);
        }
        else {
            if (this.containerProperties.getIdleEventInterval() != null) {
                long now = System.currentTimeMillis();
                if (now > lastReceive + this.containerProperties.getIdleEventInterval()
                    && now > lastAlertAt + this.containerProperties.getIdleEventInterval()) {
                    publishIdleContainerEvent(now - lastReceive, this.isConsumerAwareListener? this.consumer : null, this.consumerPaused);
                    lastAlertAt = now;
                    if (this.genericListener instanceof ConsumerSeekAware) {
                        seekPartitions(getAssignedPartitions(), true);
                    }
                }
            }
        }
    }
    catch (WakeupException e) {
        // Ignore, we're stopping
    }
    catch (NoOffsetForPartitionException nofpe) {
        this.fatalError = true;
        ListenerConsumer.this.logger.error("No offset and no reset policy", nofpe);
        break;
    }
    catch (Exception e) {
        handleConsumerException(e);
    }
}
ProducerFactoryUtils.clearConsumerGroupId();
if (!this.fatalError) {
    if (this.kafkaTxManager == null) {
        commitPendingAcks();
        try {
            this.consumer.unsubscribe();
        }
        catch (WakeupException e) {
            // No-op. Continue process
        }
    }
    else {
        closeProducers(getAssignedPartitions());
    }
}
else {
    ListenerConsumer.this.logger.error("No offset and no reset policy; stopping container");
    KafkaMessageListenerContainer.this.stop();
}
this.monitorTask.cancel(true);
if (!this.taskSchedulerExplicitlySet) {
    ((ThreadPoolTaskScheduler) this.taskScheduler).destroy();
}
this.consumer.close();
this.logger.info("Consumer stopped");
}
```

可见在listen而Consumer中有对offset进行处理的函数:processCommits，看一下其处理流程：

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#processCommits

```java
private void processCommits() {
    this.count += this.acks.size();
    // 处理队列中的ack的信息
    // 也就是把 acks中的offset信息  保存到 offsets中
    handleAcks();
    long now;
    // 获取ack的模式
    AckMode ackMode = this.containerProperties.getAckMode();
    // 如果不是立即提交
    if (!this.isManualImmediateAck) {
        // 如果不是手动提交
        if (!this.isManualAck) {
            // 则更新 offsets 中的值
            updatePendingOffsets();
        }
        // 当前的ack数量是否大于   配置的ackCount
        boolean countExceeded = this.count >= this.containerProperties.getAckCount();
        // countExceeded为真,且是ManualAck  BatchAck  RecordAck  AckMode.COUNT 这几个模式中一个
        if (this.isManualAck || this.isBatchAck || this.isRecordAck
            || (ackMode.equals(AckMode.COUNT) && countExceeded)) {
            if (this.logger.isDebugEnabled() && ackMode.equals(AckMode.COUNT)) {
                this.logger.debug("Committing in AckMode.COUNT because count " + this.count
                                  + " exceeds configured limit of " + this.containerProperties.getAckCount());
            }
            // 则进行一次提交
            commitIfNecessary();
            this.count = 0;
        }
        else {
            // 获取当前的时间
            now = System.currentTimeMillis();
            // 判断时间 间隔是否大于 配置的 ackTime
            boolean elapsed = now - this.last > this.containerProperties.getAckTime();
            // 如果ack模式是AckMode.TIME, 并且时间也够长
            if (ackMode.equals(AckMode.TIME) && elapsed) {
                if (this.logger.isDebugEnabled()) {
                    this.logger.debug("Committing in AckMode.TIME " +
                                      "because time elapsed exceeds configured limit of " +
                                      this.containerProperties.getAckTime());
                }
                // 则进行一次提交
                commitIfNecessary();
                this.last = now;
            }
            // 如果模式是根据数量或时间 AckMode.COUNT_TIME, 并且时间和数量满足其一
            else if (ackMode.equals(AckMode.COUNT_TIME) && (elapsed || countExceeded)) {
                if (this.logger.isDebugEnabled()) {
                    if (elapsed) {
                        this.logger.debug("Committing in AckMode.COUNT_TIME " +
                                          "because time elapsed exceeds configured limit of " +
                                          this.containerProperties.getAckTime());
                    }
                    else {
                        this.logger.debug("Committing in AckMode.COUNT_TIME " +
                                          "because count " + this.count + " exceeds configured limit of" + this.containerProperties.getAckCount());
                    }
                }
                // 则进行一次提交
                commitIfNecessary();
                this.last = now;
                this.count = 0;
            }
        }
    }
}
```

这里看到对offset的提交进行了各种处理，针对count数量的提交，时间的提交等模式进行了涵盖处理。由此可见除非是手动立即提交，其他情况都会保存offset进行，后续进行提交。以此可见，手动提交，仍然会有offset提交失误的情况出现。

> org.springframework.kafka.listener.KafkaMessageListenerContainer.ListenerConsumer#commitIfNecessary

```java
private void commitIfNecessary() {
    // 根据offset队列消息创建要提交的offset信息
    Map<TopicPartition, OffsetAndMetadata> commits = buildCommits();
    if (this.logger.isDebugEnabled()) {
        this.logger.debug("Commit list: " + commits);
    }
    // 要提交的信息不为空
    if (!commits.isEmpty()) {
        this.commitLogger.log(() -> "Committing: " + commits);
        try {
            // 设置的同步提交,则进行同步提交
            if (this.containerProperties.isSyncCommits()) {
                this.consumer.commitSync(commits);
            }
            else {
                // 否则进行异步提交,并设置回调方法
                this.consumer.commitAsync(commits, this.commitCallback);
            }
        }
        catch (WakeupException e) {
            // ignore - not polling
            this.logger.debug("Woken up during commit");
        }
    }
}

// 把offset队列中的信息,转换为可以提交的map
private Map<TopicPartition, OffsetAndMetadata> buildCommits() {
    Map<TopicPartition, OffsetAndMetadata> commits = new HashMap<>();
    for (Entry<String, Map<Integer, Long>> entry : this.offsets.entrySet()) {
        for (Entry<Integer, Long> offset : entry.getValue().entrySet()) {
            commits.put(new TopicPartition(entry.getKey(), offset.getKey()),
                        new OffsetAndMetadata(offset.getValue() + 1));
        }
    }
    this.offsets.clear();
    return commits;
}
```









































